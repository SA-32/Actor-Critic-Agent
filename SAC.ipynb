{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_dims, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.mu = nn.Linear(128, action_dim)\n",
    "        self.sigma = nn.Linear(128, action_dim)\n",
    "        self.max_action = max_action\n",
    "        self.noise = 1e-6\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        sigma = torch.clamp(sigma, self.noise, 1)\n",
    "\n",
    "        return mu, sigma\n",
    "    \n",
    "    def sample_normal(self, x):\n",
    "        mu, sigma = self.forward(x)\n",
    "        probabilities = Normal(mu, sigma)\n",
    "\n",
    "        actions = probabilities.sample()\n",
    "\n",
    "        action = torch.tanh(actions) * self.max_action\n",
    "\n",
    "        log_action = probabilities.log_prob(action)\n",
    "\n",
    "        log_action -= torch.log(1 - torch.tanh(actions)**2 + 1e-6)\n",
    "\n",
    "        log_action = log_action.sum(dim = -1)\n",
    "\n",
    "        return action, log_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39322e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dims, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims + action_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim = 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef269cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, in_dims):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    def __init__(self, mem_size, in_dims, action_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.state = np.zeros((mem_size, in_dims))\n",
    "        self.next_state = np.zeros((mem_size, in_dims))\n",
    "        self.action = np.zeros((mem_size, action_dims))\n",
    "        self.reward = np.zeros(mem_size)\n",
    "        self.done = np.zeros(mem_size)\n",
    "        self.counter = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        index = self.counter % self.mem_size\n",
    "\n",
    "        self.state[index] = state\n",
    "        self.next_state[index] = next_state\n",
    "        self.action[index] = action\n",
    "        self.reward[index] = reward\n",
    "        self.done[index] = done\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.counter, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, size = batch_size)\n",
    "\n",
    "        states = self.state[batch]\n",
    "        next_state = self.next_state[batch]\n",
    "        action = self.action[batch]\n",
    "        reward = self.reward[batch]\n",
    "        done = self.done[batch]\n",
    "\n",
    "        return states, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b02820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, input_dims, action_dims, max_action, tau = 0.005, mem_size = 10000, gamma = 0.99):\n",
    "        self.tau = tau\n",
    "        self.actor = Actor(input_dims, action_dims, max_action)\n",
    "        self.critic1 = Critic(input_dims, action_dims)\n",
    "        self.critic2 = Critic(input_dims, action_dims)\n",
    "        self.value = Value(input_dims)\n",
    "        self.target_value = Value(input_dims)\n",
    "\n",
    "        self.critic1_opt = torch.optim.Adam(self.critic1.parameters(), lr = 0.0003)\n",
    "        self.critic2_opt = torch.optim.Adam(self.critic2.parameters(), lr = 0.0003)\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr = 0.0003)\n",
    "        self.value_opt = torch.optim.Adam(self.value.parameters(), lr = 0.0003)\n",
    "        self.memory = Buffer(mem_size, input_dims, action_dims)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 256\n",
    "\n",
    "    def update_parameters(self, tau = None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param1, param2 in zip(self.value.parameters(), self.target_value.parameters()):\n",
    "                param2.copy_(param1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.actor.sample_normal(state)\n",
    "\n",
    "        return action.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def learn(self):\n",
    "        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        state = torch.tensor(state, dtype = torch.float32)\n",
    "        action = torch.tensor(action, dtype = torch.float32)\n",
    "        reward = torch.tensor(reward, dtype = torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32)\n",
    "        done = torch.tensor(done, dtype = torch.float32)\n",
    "\n",
    "        value = self.value(state)\n",
    "        value_ = self.target_value(next_state)\n",
    "\n",
    "        new_action, log_prob = self.actor.sample_normal(state)\n",
    "\n",
    "        critic1_pred = self.critic1(state, new_action)\n",
    "        critic2_pred = self.critic2(state, new_action)\n",
    "\n",
    "        q_value = torch.min(critic1_pred, critic2_pred)\n",
    "\n",
    "        true = q_value + log_prob\n",
    "\n",
    "        self.value_opt.zero_grad()\n",
    "\n",
    "        loss_value = self.criterion(value, true)\n",
    "\n",
    "        loss_value.backward()\n",
    "\n",
    "        self.value_opt.step()\n",
    "\n",
    "        new_action, new_prob = self.actor.sample_normal(state)\n",
    "\n",
    "        critic1_pred = self.critic1(state, new_action)\n",
    "        critic2_pred = self.critic2(state, new_action)\n",
    "\n",
    "        true = torch.min(critic1_pred, critic2_pred)\n",
    "\n",
    "        actor_loss = self.criterion(new_prob, true)\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        self.actor_opt.step()\n",
    "\n",
    "        q_hat = reward + self. gamma * value_ * (1 - done)\n",
    "\n",
    "        q_pred1 = self.critic1(state, action)\n",
    "\n",
    "        q_pred2 = self.critic2(state, action)\n",
    "\n",
    "        loss1 = self.criterion(q_pred1, q_hat)\n",
    "        loss2 = self.criterion(q_pred2, q_hat)\n",
    "\n",
    "        critic_loss = loss1 + loss2\n",
    "\n",
    "        self.critic1_opt.zero_grad()\n",
    "        self.critic2_opt.zero_grad()\n",
    "\n",
    "        critic_loss.backward()\n",
    "\n",
    "        self.critic1_opt.step()\n",
    "        self.critic2_opt.step()\n",
    "\n",
    "\n",
    "        self.update_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe70134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07979dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815949e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(input_dims = env.observation_space.shape[0], action_dims = env.action_space.shape[0], max_action= env.action_space.high)\n",
    "n_games = 1500\n",
    "score_history = []\n",
    "\n",
    "# Replace your training loop with this:\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]  # New reset returns (obs, info)\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(torch.tensor(observation).unsqueeze(0))\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # 'done' is now either terminated or truncated\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.remember(observation, action, reward, observation_[0], done)\n",
    "        \n",
    "        score += reward\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    print(f\"epoch : {i} :: score : {score} :: avg_score : {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape\n",
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f58a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62eefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.tensor([[1,2,3],\n",
    "          [2,1,5],\n",
    "          [5,6,7]])\n",
    "\n",
    "model2 = torch.tensor([[45,6,77],\n",
    "          [11,22,33],\n",
    "          [66,77,88]])\n",
    "\n",
    "for param1, param2 in zip(model1, model2):\n",
    "    param1.copy_(param2)\n",
    "\n",
    "print(model1)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e18286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
